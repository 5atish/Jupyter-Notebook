{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Machine Learning Algorithm Performance Metrics\n",
    "\n",
    "To evaluate machine learning algorithms are very important.Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose. \n",
    "\n",
    "10.1 Algorithm Evaluation Metrics\n",
    "\n",
    "Recipes evaluate the same algorithms, Logistic Regression for classification and Linear\n",
    "Regression for the regression problems. A 10-fold cross validation test harness is used to demonstrate each metric. In these recipes is the cross validation.cross_val_score function used to report the performance in each recipe. all scores are reported so that they can be sorted in ascending order (largest score is best). \n",
    "\n",
    "\n",
    "10.2 Classification Metrics\n",
    "In this section we will review how to use the below metrics.\n",
    "\n",
    "10.2.1 Classification Accuracy\n",
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions\n",
    "made. This is the most common evaluation metric for classification problems. It is really only suitable when there are an equal number of observations in each class (rare case)and all predictions and prediction errors are equally important.(often not the case).\n",
    "\n",
    "10.2.2 Logarithmic Loss\n",
    "\n",
    "Logarithmic Loss is a performance metric for evaluating the predictions of probabilities\n",
    "of membership to a given class. The scalar probability between 0 and 1 can be seen as a measure\n",
    "of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are\n",
    "rewarded or punished proportionally to the confidence of the prediction.\n",
    "\n",
    "10.2.3 Area Under ROC Curve (AUC)\n",
    "\n",
    "Area Under ROC Curve is a performance metric for binary classificationproblems. The AUC represents a model's ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. ROC can be broken down into sensitivity and specificity. \n",
    "Sensitivity is the true positive rate also called the recall. It is the number of instances\n",
    "from the positive (first) class that actually predicted correctly.\n",
    "Specificity is also called the true negative rate. Is the number of instances from the\n",
    "negative (second) class that were actually predicted correctly.\n",
    "\n",
    "10.2.4 Confusion Matrix\n",
    "\n",
    "Confusion Matrix presentation of the accuracy of a model with two or more classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm. \n",
    "e.g. a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction =0 and actual = 1, and so on. \n",
    "\n",
    "10.2.5 Classification Report\n",
    "\n",
    "Classification Report while working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.87      0.82       162\n",
      "        1.0       0.71      0.55      0.62        92\n",
      "\n",
      "avg / total       0.75      0.76      0.75       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "\n",
    "################ Cross Validation Classification Accuracy ############################\n",
    "\n",
    "# scoring = 'accuracy'\n",
    "# result = cross_val_score(model, x, y, cv = kfold, scoring = scoring)\n",
    "# print(\"Mean Accuracy: \", result.mean()*100)\n",
    "# print(\"Standard Deviation Accuracy: \", result.std()*100)\n",
    "# #### Mean Accuracy: 76.95, std Accuracy:4.84\n",
    "\n",
    "################ Cross Validation Classification using Logarithmic Loss ###############\n",
    "\n",
    "# scoring = 'neg_log_loss'\n",
    "# result = cross_val_score(model, x, y, cv = kfold, scoring = scoring)\n",
    "# print(\"Mean Accuracy: \", result.mean()*100)\n",
    "# print(\"Standard Deviation Accuracy: \", result.std()*100)\n",
    "#### Mean Accuracy: -49.26, std Accuracy:4.68\n",
    "#### Smaller logloss is better with 0 representing a perfect logloss. From above, \n",
    "#### the measure is inverted to be ascending when using the cross val score() function.\n",
    "\n",
    "################ Cross Validation Classification using Area Under ROC Curve (AUC) ######\n",
    "\n",
    "# scoring = 'roc_auc'\n",
    "# result = cross_val_score(model, x, y, cv = kfold, scoring = scoring)\n",
    "# print(\"Mean Accuracy: \", result.mean()*100)\n",
    "# print(\"Standard Deviation Accuracy: \", result.std()*100)\n",
    "#### Mean Accuracy: 82.34, std Accuracy:4.07\n",
    "#### AUC is relatively close to 1 and greater than 0.5, suggesting some skill in predictions\n",
    "\n",
    "################ Cross Validation Classification using Confusion Matrix ###############\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = test_size, random_state = seed)\n",
    "model.fit(x_train,y_train)\n",
    "predict = model.predict(x_test)\n",
    "# matrix = confusion_matrix(y_test, predict)\n",
    "# print(matrix)\n",
    "\n",
    "### the majority of predictions fall on the diagonal line of the matrix i.e.correct predictions.\n",
    "\n",
    "################ Cross Validation Classification using Classification Report ############\n",
    "\n",
    "report = classification_report(y_test, predict)\n",
    "print(report)\n",
    "### good prediction and recall for the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3 Regression Metrics\n",
    "\n",
    "Regression metrics, the Boston House Price dataset is used to demonstrate egression problem where all of the input variables are also numeric. Below are the most common metrics for evaluating predictions on regression machine learning. \n",
    "problems.\n",
    "\n",
    "10.3.1 Mean Absolute Error (MAE)\n",
    "\n",
    "The Mean Absolute Error is the sum of the absolute differences between predictions\n",
    "and actual values. It gives an idea of how wrong the predictions were. The measure gives an\n",
    "idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n",
    "\n",
    "10.3.2 Mean Squared Error (MSE) \n",
    "\n",
    "Mean Squared Error is much like the mean absolute error in that it provides a gross idea of the magnitude of error. Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE). \n",
    "\n",
    "\n",
    "10.3.2 R Squared Metric (R^2)\n",
    "\n",
    "This metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature this measure is called the coeficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.20252899006056085\n",
      "STD:  0.5952960169512264\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('C:/Users/Satish/python_files/housing.csv', delim_whitespace=True, names=names)\n",
    "# print(data.head(10))\n",
    "array = data.values\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "\n",
    "############# Cross Validation Regression using Mean Absolute Error #################\n",
    "\n",
    "# scoring = 'neg_mean_absolute_error'\n",
    "# result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(\"Mean: \", result.mean())\n",
    "# print(\"STD: \", result.std())\n",
    "### Mean:-4.00    STD:2.08\n",
    "### A value of 0 indicates no error or perfect predictions. Metric is inverted by the \n",
    "### cross val score() function.\n",
    "\n",
    "############# Cross Validation Regression using Mean Squared Error #################\n",
    "\n",
    "# scoring = 'neg_mean_squared_error'\n",
    "# result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(\"Mean: \", result.mean())\n",
    "# print(\"STD: \", result.std())\n",
    "### Mean:-34.70    STD:45.57\n",
    "### This metric too is inverted so that the results are increasing. \n",
    "\n",
    "############# Cross Validation Regression using R^2 ##################################\n",
    "\n",
    "scoring = 'r2'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "print(\"Mean: \", result.mean())\n",
    "print(\"STD: \", result.std())\n",
    "### Mean:0.20    STD:0.59\n",
    "### the predictions have a poor \f",
    "t to the actual values with a value closer to zero and less \n",
    "### than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Spot-Check Classification Algorithms\n",
    "\n",
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning\n",
    "problem. You cannot know which algorithms are best suited to your problem beforehand. You must trial a number of methods and focus attention on those that prove themselves the most promising. \n",
    "\n",
    "11.1 Algorithm Spot-Checking\n",
    "\n",
    "We cannot know which algorithm will work best on your dataset beforehand. We must use trial and error to discover a shortlist of algorithms that do well on the problem that we can then double down on and tune further. We can guess at what algorithms might do well on the dataset, and this can be a good starting point. Try a mixture of algorithms and see what is good at picking out the structure in the data. \n",
    "Below are some suggestions when spot-checking algorithms on your dataset:\n",
    "1} Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "2} Try a mixture of learning algorithms (e.g. different algorithms for learning the same type\n",
    "   of representation).\n",
    "3} Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and\n",
    "   nonparametric).\n",
    "\n",
    "Each recipe is demonstrated on the Pima Indians onset of Diabetes dataset. A test harness\n",
    "using 10-fold cross validation is used to demonstrate how to spot-check each machine learning\n",
    "algorithm and mean accuracy measures are used to indicate algorithm performance. \n",
    "\n",
    "11.3 Linear Machine Learning Algorithms\n",
    "\n",
    "This section demonstrates recipes for how to use two linear machine learning algorithms i.e. logistic regression and linear discriminant analysis.\n",
    "\n",
    "11.3.1 Logistic Regression\n",
    "\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can\n",
    "model binary classification problems. \n",
    "\n",
    "11.3.2 Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\n",
    "classification. It too assumes a Gaussian distribution for the numerical input variables. \n",
    "\n",
    "\n",
    "11.4 Nonlinear Machine Learning Algorithms\n",
    "\n",
    "Recipes for how to use 4 nonlinear machine learning algorithms.\n",
    "\n",
    "11.4.1 k-Nearest Neighbors (KNN)\n",
    "\n",
    "The k-Nearest Neighbors algorithm uses a distance metric to find the k most similar\n",
    "instances in the training data for a new instance and takes the mean outcome of the neighbors\n",
    "as the prediction. \n",
    "\n",
    "11.4.2 Naive Bayes\n",
    "\n",
    "Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function. \n",
    "\n",
    "11.4.3 Classification and Regression Trees (CART)\n",
    "\n",
    "Classification and Regression Trees (CART or just decision trees) construct a binary tree from\n",
    "the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index)\n",
    "\n",
    "11.4.4 Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510252904989747\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "######################## Logistic Regression Classification #############################\n",
    "model = LogisticRegression()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.769\n",
    "\n",
    "######################## Linear Discriminant Analysis ####################################\n",
    "model = LinearDiscriminantAnalysis()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.773\n",
    "\n",
    "######################## k-Nearest Neighbors Classification #############################\n",
    "model  = KNeighborsClassifier()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.726\n",
    "\n",
    "######################## Gaussian Naive Bayes Classification #############################\n",
    "model  = GaussianNB()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.755\n",
    "\n",
    "######################### CART Classification ############################################\n",
    "model = DecisionTreeClassifier()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.699\n",
    "\n",
    "######################## Support Vector Machines Classification ##########################\n",
    "model = SVC()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "print(result.mean())\n",
    "### Mean estimated accuracy = 0.651\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Spot-Check Regression Algorithms\n",
    "\n",
    "Spot-checking is a way of discovering which algorithms perform well on machine learning problem. We cannot know which algorithms are best suited to the problem beforehand. We must trial a number of methods and focus attention on those that prove themselves the most promising. In this we will discover six machine learning algorithms that you can use when spot-checking your regression problem in Python with scikit-learn. \n",
    "\n",
    "12.2 Linear Machine Learning Algorithms\n",
    "\n",
    "This section provides how to use four different linear machine learning algorithms for regression in Python with scikit-learn.\n",
    "\n",
    "12.2.1 Linear Regression\n",
    "\n",
    "Linear regression assumes that the input variables have a Gaussian distribution. It is also\n",
    "assumed that input variables are relevant to the output variable and that they are not highly\n",
    "correlated with each other \n",
    "\n",
    "12.2.2 Ridge Regression\n",
    "\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to\n",
    "minimize the complexity of the model measured as the sum squared value of the coeficient values (also called the L2-norm). \n",
    "\n",
    "12.2.3 LASSO Regression\n",
    "\n",
    "The Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modification\n",
    "of linear regression, like ridge regression, where the loss function is modified to minimize the complexity of the model measured as the sum absolute value of the coeficient values (also called the L1-norm).\n",
    "\n",
    "12.2.4 ElasticNet Regression\n",
    "\n",
    "ElasticNet is a form of regularization regression that combines the properties of both Ridge\n",
    "Regression and LASSO regression. It seeks to minimize the complexity of the regression model\n",
    "(magnitude and number of regression coeficients) by penalizing the model using both the\n",
    "L2-norm (sum squared coeficient values) and the L1-norm (sum absolute coeficient values).\n",
    "\n",
    "12.3 Nonlinear Machine Learning Algorithms\n",
    "\n",
    "This section provides examples of how to use three different nonlinear machine learning algorithms for regression in Python with scikit-learn.\n",
    "\n",
    "12.3.1 K-Nearest Neighbors (KNN)\n",
    "\n",
    "The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
    "training dataset for a new data instance. From the k neighbors, a mean or median output\n",
    "variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
    "The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
    "distance (used when all inputs have the same scale) and Manhattan distance (for when the\n",
    "scales of the input variables differ). \n",
    "\n",
    "12.3.2 Classification and Regression Trees (CART)\n",
    "\n",
    "CART use the training data to select the best points to split the data in order to minimize a cost metric. The default cost metric for regression decision trees is the mean squared error, specified in the criterion parameter. \n",
    "\n",
    "12.3.3 Support Vector Machines (SVM)\n",
    "\n",
    "upport Vector Machines (SVM) were developed for binary classification. The technique has\n",
    "been extended for the prediction real-valued problems called Support Vector Regression (SVR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.04782433324428\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('C:/Users/Satish/python_files/housing.csv', delim_whitespace=True, names=names)\n",
    "# print(data.head(10))\n",
    "array = data.values\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "############################ Linear Regression  #######################################\n",
    "\n",
    "model = LinearRegression()\n",
    "scoring  = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.70\n",
    "\n",
    "############################ Ridge Regression  #######################################\n",
    "\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.07\n",
    "\n",
    "\n",
    "############################ Lasso Regression  #######################################\n",
    "\n",
    "model = Lasso()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.46\n",
    "\n",
    "############################ ElasticNet Regression  #######################################\n",
    "\n",
    "model = ElasticNet()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.16\n",
    "\n",
    "############################ KNN Regression  #######################################\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -107.28\n",
    "\n",
    "############################ CART Regression  #######################################\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -35.65\n",
    "\n",
    "############################ SVR Regression  #######################################\n",
    "\n",
    "model = SVR()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "print(result.mean())\n",
    "### Mean estimated accuracy = -91.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Compare Machine Learning Algorithms\n",
    "\n",
    "It is important to compare the performance of multiple different machine learning algorithms\n",
    "consistently. In this chapter you will discover how you can create a test harness to compare\n",
    "multiple different machine learning algorithms in Python with scikit-learn. You can use this\n",
    "test harness as a template on your own machine learning problems and add more and different\n",
    "algorithms to compare. \n",
    "\n",
    "13.1 Choose The Best Machine Learning Model\n",
    "\n",
    "While working on machine learning project, we often end up with multiple good models to choose from and each model will have different performance characteristics.Using resampling methods like cross validation, you can get an estimate for how accurate each model may be on unseen data. We need to be able to use these estimates to choose one or two best models from the suite of models that we have created.\n",
    "When we have a new dataset, it is a good idea to visualize the data using different techniques\n",
    "in order to look at the data from different perspectives. The same idea applies to model selection. We should use a number of different ways of looking at the estimated accuracy of  machine learning algorithms in order to choose the one or two algorithm to finalize.\n",
    "\n",
    "13.2 Compare Machine Learning Algorithms Consistently\n",
    "\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is\n",
    "evaluated in the same way on the same data. This can be achieve by forcing each algorithm to be evaluated on a consistent test harness. \n",
    "In the example below six different classification algorithms are compared on a single dataset using diabetes dataset which has two classes and eight numeric input variables of varying scales. The 10-fold cross validation procedure is used to evaluate each algorithm with the same random seed to ensure that the same splits to the training data are performed and that each algorithm is evaluated in precisely the same way. \n",
    "\n",
    "1} Logistic Regression.\n",
    "2} Linear Discriminant Analysis.\n",
    "3} k-Nearest Neighbors.\n",
    "4} Classification and Regression Trees.\n",
    "5} Naive Bayes.\n",
    "6} Support Vector Machines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.7695146958304853 0.04841051924567195\n",
      "LDA 0.773462064251538 0.05159180390446138\n",
      "KNN 0.7265550239234451 0.06182131406705549\n",
      "GNB 0.7551777170198223 0.04276593954064409\n",
      "CART 0.6887218045112782 0.05807401623431069\n",
      "SVM 0.6510252904989747 0.07214083485055327\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "\n",
    "#### Prepare model\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "#### EValuate each model in turn \n",
    "result = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "    result.append(cv_result)\n",
    "    names.append(name)\n",
    "    print(name, cv_result.mean(), cv_result.std())\n",
    "    \n",
    "#box plot algorithm comparison    \n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(result)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()\n",
    "\n",
    "#### From these results, it would suggest that both logistic regression and linear discriminant\n",
    "#### analysis are perhaps worthy of further study on this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
