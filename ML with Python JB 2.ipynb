{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Machine Learning Algorithm Performance Metrics\n",
    "\n",
    "To evaluate machine learning algorithms are very important.Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose. \n",
    "\n",
    "10.1 Algorithm Evaluation Metrics\n",
    "\n",
    "Recipes evaluate the same algorithms, Logistic Regression for classification and Linear\n",
    "Regression for the regression problems. A 10-fold cross validation test harness is used to demonstrate each metric. In these recipes is the cross validation.cross_val_score function used to report the performance in each recipe. all scores are reported so that they can be sorted in ascending order (largest score is best). \n",
    "\n",
    "\n",
    "10.2 Classification Metrics\n",
    "In this section we will review how to use the below metrics.\n",
    "\n",
    "10.2.1 Classification Accuracy\n",
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions\n",
    "made. This is the most common evaluation metric for classification problems. It is really only suitable when there are an equal number of observations in each class (rare case)and all predictions and prediction errors are equally important.(often not the case).\n",
    "\n",
    "10.2.2 Logarithmic Loss\n",
    "\n",
    "Logarithmic Loss is a performance metric for evaluating the predictions of probabilities\n",
    "of membership to a given class. The scalar probability between 0 and 1 can be seen as a measure\n",
    "of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are\n",
    "rewarded or punished proportionally to the confidence of the prediction.\n",
    "\n",
    "10.2.3 Area Under ROC Curve (AUC)\n",
    "\n",
    "Area Under ROC Curve is a performance metric for binary classificationproblems. The AUC represents a model's ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. ROC can be broken down into sensitivity and specificity. \n",
    "Sensitivity is the true positive rate also called the recall. It is the number of instances\n",
    "from the positive (first) class that actually predicted correctly.\n",
    "Specificity is also called the true negative rate. Is the number of instances from the\n",
    "negative (second) class that were actually predicted correctly.\n",
    "\n",
    "10.2.4 Confusion Matrix\n",
    "\n",
    "Confusion Matrix presentation of the accuracy of a model with two or more classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm. \n",
    "e.g. a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction =0 and actual = 1, and so on. \n",
    "\n",
    "10.2.5 Classification Report\n",
    "\n",
    "Classification Report while working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.87      0.82       162\n",
      "        1.0       0.71      0.55      0.62        92\n",
      "\n",
      "avg / total       0.75      0.76      0.75       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "\n",
    "################ Cross Validation Classification Accuracy ############################\n",
    "\n",
    "# scoring = 'accuracy'\n",
    "# result = cross_val_score(model, x, y, cv = kfold, scoring = scoring)\n",
    "# print(\"Mean Accuracy: \", result.mean()*100)\n",
    "# print(\"Standard Deviation Accuracy: \", result.std()*100)\n",
    "# #### Mean Accuracy: 76.95, std Accuracy:4.84\n",
    "\n",
    "################ Cross Validation Classification using Logarithmic Loss ###############\n",
    "\n",
    "# scoring = 'neg_log_loss'\n",
    "# result = cross_val_score(model, x, y, cv = kfold, scoring = scoring)\n",
    "# print(\"Mean Accuracy: \", result.mean()*100)\n",
    "# print(\"Standard Deviation Accuracy: \", result.std()*100)\n",
    "#### Mean Accuracy: -49.26, std Accuracy:4.68\n",
    "#### Smaller logloss is better with 0 representing a perfect logloss. From above, \n",
    "#### the measure is inverted to be ascending when using the cross val score() function.\n",
    "\n",
    "################ Cross Validation Classification using Area Under ROC Curve (AUC) ######\n",
    "\n",
    "# scoring = 'roc_auc'\n",
    "# result = cross_val_score(model, x, y, cv = kfold, scoring = scoring)\n",
    "# print(\"Mean Accuracy: \", result.mean()*100)\n",
    "# print(\"Standard Deviation Accuracy: \", result.std()*100)\n",
    "#### Mean Accuracy: 82.34, std Accuracy:4.07\n",
    "#### AUC is relatively close to 1 and greater than 0.5, suggesting some skill in predictions\n",
    "\n",
    "################ Cross Validation Classification using Confusion Matrix ###############\n",
    "\n",
    "# test_size = 0.33\n",
    "# seed = 7\n",
    "# x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = test_size, random_state = seed)\n",
    "# model.fit(x_train,y_train)\n",
    "# predict = model.predict(x_test)\n",
    "# matrix = confusion_matrix(y_test, predict)\n",
    "# print(matrix)\n",
    "\n",
    "### the majority of predictions fall on the diagonal line of the matrix i.e.correct predictions.\n",
    "\n",
    "################ Cross Validation Classification using Classification Report ############\n",
    "\n",
    "report = classification_report(y_test, predict)\n",
    "print(report)\n",
    "### good prediction and recall for the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3 Regression Metrics\n",
    "\n",
    "Regression metrics, the Boston House Price dataset is used to demonstrate egression problem where all of the input variables are also numeric. Below are the most common metrics for evaluating predictions on regression machine learning. \n",
    "problems.\n",
    "\n",
    "10.3.1 Mean Absolute Error (MAE)\n",
    "\n",
    "The Mean Absolute Error is the sum of the absolute differences between predictions\n",
    "and actual values. It gives an idea of how wrong the predictions were. The measure gives an\n",
    "idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n",
    "\n",
    "10.3.2 Mean Squared Error (MSE) \n",
    "\n",
    "Mean Squared Error is much like the mean absolute error in that it provides a gross idea of the magnitude of error. Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE). \n",
    "\n",
    "\n",
    "10.3.2 R Squared Metric (R^2)\n",
    "\n",
    "This metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature this measure is called the coeficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.20252899006056085\n",
      "STD:  0.5952960169512264\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('C:/Users/Satish/python_files/housing.csv', delim_whitespace=True, names=names)\n",
    "# print(data.head(10))\n",
    "array = data.values\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "\n",
    "############# Cross Validation Regression using Mean Absolute Error #################\n",
    "\n",
    "# scoring = 'neg_mean_absolute_error'\n",
    "# result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(\"Mean: \", result.mean())\n",
    "# print(\"STD: \", result.std())\n",
    "\n",
    "### Mean:-4.00    STD:2.08\n",
    "### A value of 0 indicates no error or perfect predictions. Metric is inverted by the \n",
    "### cross val score() function.\n",
    "\n",
    "############# Cross Validation Regression using Mean Squared Error #################\n",
    "\n",
    "# scoring = 'neg_mean_squared_error'\n",
    "# result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(\"Mean: \", result.mean())\n",
    "# print(\"STD: \", result.std())\n",
    "### Mean:-34.70    STD:45.57\n",
    "### This metric too is inverted so that the results are increasing. \n",
    "\n",
    "############# Cross Validation Regression using R^2 ##################################\n",
    "\n",
    "# scoring = 'r2'\n",
    "# result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(\"Mean: \", result.mean())\n",
    "# print(\"STD: \", result.std())\n",
    "### Mean:0.20    STD:0.59\n",
    "### the predictions have a poor fit to the actual values with a value closer to zero and less \n",
    "### than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Spot-Check Classification Algorithms\n",
    "\n",
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning\n",
    "problem. You cannot know which algorithms are best suited to your problem beforehand. You must trial a number of methods and focus attention on those that prove themselves the most promising. \n",
    "\n",
    "11.1 Algorithm Spot-Checking\n",
    "\n",
    "We cannot know which algorithm will work best on your dataset beforehand. We must use trial and error to discover a shortlist of algorithms that do well on the problem that we can then double down on and tune further. We can guess at what algorithms might do well on the dataset, and this can be a good starting point. Try a mixture of algorithms and see what is good at picking out the structure in the data. \n",
    "Below are some suggestions when spot-checking algorithms on your dataset:\n",
    "1} Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "2} Try a mixture of learning algorithms (e.g. different algorithms for learning the same type\n",
    "   of representation).\n",
    "3} Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and\n",
    "   nonparametric).\n",
    "\n",
    "Each recipe is demonstrated on the Pima Indians onset of Diabetes dataset. A test harness\n",
    "using 10-fold cross validation is used to demonstrate how to spot-check each machine learning\n",
    "algorithm and mean accuracy measures are used to indicate algorithm performance. \n",
    "\n",
    "11.3 Linear Machine Learning Algorithms\n",
    "\n",
    "This section demonstrates recipes for how to use two linear machine learning algorithms i.e. logistic regression and linear discriminant analysis.\n",
    "\n",
    "11.3.1 Logistic Regression\n",
    "\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can\n",
    "model binary classification problems. \n",
    "\n",
    "11.3.2 Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\n",
    "classification. It too assumes a Gaussian distribution for the numerical input variables. \n",
    "\n",
    "\n",
    "11.4 Nonlinear Machine Learning Algorithms\n",
    "\n",
    "Recipes for how to use 4 nonlinear machine learning algorithms.\n",
    "\n",
    "11.4.1 k-Nearest Neighbors (KNN)\n",
    "\n",
    "The k-Nearest Neighbors algorithm uses a distance metric to find the k most similar\n",
    "instances in the training data for a new instance and takes the mean outcome of the neighbors\n",
    "as the prediction. \n",
    "\n",
    "11.4.2 Naive Bayes\n",
    "\n",
    "Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function. \n",
    "\n",
    "11.4.3 Classification and Regression Trees (CART)\n",
    "\n",
    "Classification and Regression Trees (CART or just decision trees) construct a binary tree from\n",
    "the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index)\n",
    "\n",
    "11.4.4 Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "######################## Logistic Regression Classification #############################\n",
    "model = LogisticRegression()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.769\n",
    "\n",
    "######################## Linear Discriminant Analysis ####################################\n",
    "model = LinearDiscriminantAnalysis()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.773\n",
    "\n",
    "######################## k-Nearest Neighbors Classification #############################\n",
    "model  = KNeighborsClassifier()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.726\n",
    "\n",
    "######################## Gaussian Naive Bayes Classification #############################\n",
    "model  = GaussianNB()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.755\n",
    "\n",
    "######################### CART Classification ############################################\n",
    "model = DecisionTreeClassifier()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.699\n",
    "\n",
    "######################## Support Vector Machines Classification ##########################\n",
    "model = SVC()\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = 0.651\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Spot-Check Regression Algorithms\n",
    "\n",
    "Spot-checking is a way of discovering which algorithms perform well on machine learning problem. We cannot know which algorithms are best suited to the problem beforehand. We must trial a number of methods and focus attention on those that prove themselves the most promising. In this we will discover six machine learning algorithms that you can use when spot-checking your regression problem in Python with scikit-learn. \n",
    "\n",
    "12.2 Linear Machine Learning Algorithms\n",
    "\n",
    "This section provides how to use four different linear machine learning algorithms for regression in Python with scikit-learn.\n",
    "\n",
    "12.2.1 Linear Regression\n",
    "\n",
    "Linear regression assumes that the input variables have a Gaussian distribution. It is also\n",
    "assumed that input variables are relevant to the output variable and that they are not highly\n",
    "correlated with each other \n",
    "\n",
    "12.2.2 Ridge Regression\n",
    "\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to\n",
    "minimize the complexity of the model measured as the sum squared value of the coeficient values (also called the L2-norm). \n",
    "\n",
    "12.2.3 LASSO Regression\n",
    "\n",
    "The Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modification\n",
    "of linear regression, like ridge regression, where the loss function is modified to minimize the complexity of the model measured as the sum absolute value of the coeficient values (also called the L1-norm).\n",
    "\n",
    "12.2.4 ElasticNet Regression\n",
    "\n",
    "ElasticNet is a form of regularization regression that combines the properties of both Ridge\n",
    "Regression and LASSO regression. It seeks to minimize the complexity of the regression model\n",
    "(magnitude and number of regression coeficients) by penalizing the model using both the\n",
    "L2-norm (sum squared coeficient values) and the L1-norm (sum absolute coeficient values).\n",
    "\n",
    "12.3 Nonlinear Machine Learning Algorithms\n",
    "\n",
    "This section provides examples of how to use three different nonlinear machine learning algorithms for regression in Python with scikit-learn.\n",
    "\n",
    "12.3.1 K-Nearest Neighbors (KNN)\n",
    "\n",
    "The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
    "training dataset for a new data instance. From the k neighbors, a mean or median output\n",
    "variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
    "The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
    "distance (used when all inputs have the same scale) and Manhattan distance (for when the\n",
    "scales of the input variables differ). \n",
    "\n",
    "12.3.2 Classification and Regression Trees (CART)\n",
    "\n",
    "CART use the training data to select the best points to split the data in order to minimize a cost metric. The default cost metric for regression decision trees is the mean squared error, specified in the criterion parameter. \n",
    "\n",
    "12.3.3 Support Vector Machines (SVM)\n",
    "\n",
    "upport Vector Machines (SVM) were developed for binary classification. The technique has\n",
    "been extended for the prediction real-valued problems called Support Vector Regression (SVR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.04782433324428\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('C:/Users/Satish/python_files/housing.csv', delim_whitespace=True, names=names)\n",
    "# print(data.head(10))\n",
    "array = data.values\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "############################ Linear Regression  #######################################\n",
    "\n",
    "model = LinearRegression()\n",
    "scoring  = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.70\n",
    "\n",
    "############################ Ridge Regression  #######################################\n",
    "\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.07\n",
    "\n",
    "\n",
    "############################ Lasso Regression  #######################################\n",
    "\n",
    "model = Lasso()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.46\n",
    "\n",
    "############################ ElasticNet Regression  #######################################\n",
    "\n",
    "model = ElasticNet()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -34.16\n",
    "\n",
    "############################ KNN Regression  #######################################\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -107.28\n",
    "\n",
    "############################ CART Regression  #######################################\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "# print(result.mean())\n",
    "### Mean estimated accuracy = -35.65\n",
    "\n",
    "############################ SVR Regression  #######################################\n",
    "\n",
    "model = SVR()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "print(result.mean())\n",
    "### Mean estimated accuracy = -91.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Compare Machine Learning Algorithms\n",
    "\n",
    "It is important to compare the performance of multiple different machine learning algorithms\n",
    "consistently. In this chapter you will discover how you can create a test harness to compare\n",
    "multiple different machine learning algorithms in Python with scikit-learn. You can use this\n",
    "test harness as a template on your own machine learning problems and add more and different\n",
    "algorithms to compare. \n",
    "\n",
    "13.1 Choose The Best Machine Learning Model\n",
    "\n",
    "While working on machine learning project, we often end up with multiple good models to choose from and each model will have different performance characteristics.Using resampling methods like cross validation, you can get an estimate for how accurate each model may be on unseen data. We need to be able to use these estimates to choose one or two best models from the suite of models that we have created.\n",
    "When we have a new dataset, it is a good idea to visualize the data using different techniques\n",
    "in order to look at the data from different perspectives. The same idea applies to model selection. We should use a number of different ways of looking at the estimated accuracy of  machine learning algorithms in order to choose the one or two algorithm to finalize.\n",
    "\n",
    "13.2 Compare Machine Learning Algorithms Consistently\n",
    "\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is\n",
    "evaluated in the same way on the same data. This can be achieve by forcing each algorithm to be evaluated on a consistent test harness. \n",
    "In the example below six different classification algorithms are compared on a single dataset using diabetes dataset which has two classes and eight numeric input variables of varying scales. The 10-fold cross validation procedure is used to evaluate each algorithm with the same random seed to ensure that the same splits to the training data are performed and that each algorithm is evaluated in precisely the same way. \n",
    "\n",
    "1} Logistic Regression.\n",
    "2} Linear Discriminant Analysis.\n",
    "3} k-Nearest Neighbors.\n",
    "4} Classification and Regression Trees.\n",
    "5} Naive Bayes.\n",
    "6} Support Vector Machines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.7695146958304853 0.04841051924567195\n",
      "LDA 0.773462064251538 0.05159180390446138\n",
      "KNN 0.7265550239234451 0.06182131406705549\n",
      "GNB 0.7551777170198223 0.04276593954064409\n",
      "CART 0.6900034176349965 0.06249622191574835\n",
      "SVM 0.6510252904989747 0.07214083485055327\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "\n",
    "#### Prepare model\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "#### EValuate each model in turn \n",
    "result = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_result = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n",
    "    result.append(cv_result)\n",
    "    names.append(name)\n",
    "    print(name, cv_result.mean(), cv_result.std())\n",
    "    \n",
    "#box plot algorithm comparison    \n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(result)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()\n",
    "\n",
    "#### From these results, it would suggest that both logistic regression and linear discriminant\n",
    "#### analysis are perhaps worthy of further study on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Automate ML Workflows with Pipelines\n",
    "\n",
    "There are standard workflows in a machine learning project that can be automated. In Python\n",
    "scikit-learn, Pipelines help to clearly define and automate these workflows. \n",
    "\n",
    "14.1 Automating Machine Learning Workflows\n",
    "\n",
    "There are standard workflows in applied LM to overcome common problems like data leakage in the test harness. Python scikit-learn provides a Pipeline utility to help automate machine learning workflows by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated.\n",
    "The goal is to ensure that all of the steps in the pipeline are constrained to the data available for the evaluation, such as the training dataset or each fold of the cross validation procedure.\n",
    "\n",
    "14.2 Data Preparation and Modeling Pipeline\n",
    "\n",
    "An easy trap to fall into in applied ML is leaking data from training dataset to test dataset. \n",
    "To avoid this trap we need a robust test harness with strong separation of training and testing including data preparation. Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm. \n",
    "e.g. preparing your data using normalization or standardization on the entire training dataset before learning would not be a valid test because the training dataset would have been influenced by the scale of the data in the test set. \n",
    "Pipelines help to prevent data leakage in the test harness by ensuring that data preparation\n",
    "like standardization is constrained to each fold of cross validation procedure. The example below demonstrates this important data preparation and model evaluation workflow on the diabetes dataset. \n",
    "The pipeline is defined with two steps\n",
    "1} Standardize the data.\n",
    "2} Learn a Linear Discriminant Analysis model.\n",
    "The pipeline is then evaluated using 10-fold cross validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "####### Create a pipeline that standardizes the data then creates a model ###########\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "### Create the Pipeline with list of steps\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators) # provide list of steps to Pipeline for process the data\n",
    "\n",
    "### Evaluate the Pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "print(result.mean())\n",
    "#### Mean Accuracy = 0.77\n",
    "#### The Pipeline itself is treated like an estimator and evaluated entirety by the k-fold \n",
    "#### cross validation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.3 Feature Extraction and Modeling Pipeline\n",
    "\n",
    "Feature extraction is procedure that is susceptible to data leakage. Like data preparation,\n",
    "feature extraction procedures must be restricted to the data in training dataset. The pipeline provides a handy tool called the FeatureUnion which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a model can be trained. Importantly, all the feature extraction and the feature union occurs within each fold of the cross validation procedure. \n",
    "The example below demonstrates the pipeline defined with four steps\n",
    "1} Feature Extraction with Principal Component Analysis (3 features).\n",
    "2} Feature Extraction with Statistical Selection (6 features).\n",
    "3} Feature Union.\n",
    "4} Learn a Logistic Regression Model.\n",
    "The pipeline is then evaluated using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7760423786739576\n"
     ]
    }
   ],
   "source": [
    "####### Create a pipeline that extracts feature from the data then creates a model ###########\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "### create feature Union \n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3))) # step 1 Feature extraction with PCA, 3 component\n",
    "features.append(('select_best', SelectKBest(k=6))) #step 2 Feature selection with selectKbest\n",
    "feature_union = FeatureUnion(features) #FeatureUnion is it's own Pipeline of PCA and selectKbest\n",
    "\n",
    "### create Pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union)) # FeatureUnion feed to  final pipeline\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators) # Final pipeline with FeatureUnion and  LogisticRegression\n",
    "\n",
    "### Evaluate the Pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "print(result.mean())\n",
    "### Mean Accuracy =  0.776"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Improve Performance with Ensembles\n",
    "\n",
    "Ensembles can give a boost in accuracy on the dataset. \n",
    "\n",
    "15.1 Combine Models Into Ensemble Predictions\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "1} Bagging -  Building multiple models (typically of the same type) from different subsamples\n",
    "              of the training dataset.\n",
    "2} Boosting - Building multiple models (typically of the same type) each of which learns to\n",
    "              fix the prediction errors of a prior model in the sequence of models.\n",
    "3} Voting - Building multiple models (typically of differing types) and simple statistics (like\n",
    "            calculating the mean) are used to combine predictions.\n",
    "            \n",
    "15.2 Bagging Algorithms\n",
    "\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n",
    "(with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. \n",
    "\n",
    "15.2.1 Bagged Decision Trees\n",
    "\n",
    "Bagging performs best with algorithms that have high variance. A popular example are\n",
    "decision trees, often constructed without pruning. \n",
    "The example below is an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created.\n",
    "\n",
    "15.2.2 Random Forest\n",
    "\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are\n",
    "taken with replacement, but the trees are constructed in a way that reduces the correlation\n",
    "between individual classiers. Rather than greedily choosing the best split point in\n",
    "the construction of each tree, only a random subset of features are considered for each split. \n",
    "\n",
    "15.2.3 Extra Trees\n",
    "\n",
    "Extra Trees are modification of bagging where random trees are constructed from samples of the training dataset. \n",
    "\n",
    "15.3 Boosting Algorithms\n",
    "\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\n",
    "of the models before them in the sequence. Once created, the models make predictions which\n",
    "may be weighted by their demonstrated accuracy and the results are combined to create a final\n",
    "output prediction. \n",
    "\n",
    "15.3.1 AdaBoost\n",
    "\n",
    "AdaBoost works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. \n",
    "\n",
    "15.3.2 Stochastic Gradient Boosting\n",
    "\n",
    "tochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\n",
    "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\n",
    "the best techniques available for improving performance via ensembles. You can construct a\n",
    "Gradient Boosting model for classification using the GradientBoostingClassifier class.\n",
    "    \n",
    "15.4 Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning\n",
    "algorithms. It works by first creating two or more standalone models from training dataset.\n",
    "A Voting Classifier can then be used to wrap the models and average the predictions of the\n",
    "sub-models when asked to make predictions for new data. The predictions of the sub-models can\n",
    "be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from sub-models, but this is called stacking (stacked aggregation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669002050580999\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "################# Bagged Decision Trees for Classification ####################\n",
    "# num_trees = 100\n",
    "# cart = DecisionTreeClassifier()\n",
    "# model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "# result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Estimated mean Accuracy = 0.77\n",
    "\n",
    "#################### Random Forest Classification ##############################\n",
    "# max_feature = 3\n",
    "# num_trees = 100\n",
    "# model = RandomForestClassifier(n_estimators=num_trees, max_features=max_feature)\n",
    "# result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Estimated mean Accuracy = 0.772\n",
    "\n",
    "#################### Extra trees Classification ##############################\n",
    "# max_feature = 7\n",
    "# num_trees = 100\n",
    "# model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_feature)\n",
    "# result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Estimated mean Accuracy = 0.755\n",
    "\n",
    "#################### AdaBoost Classification ####################################\n",
    "# max_feature = 7\n",
    "# model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_feature)\n",
    "# result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Estimated mean Accuracy = 0.772\n",
    "\n",
    "#################### Stochastic Gradient Boosting Classification #####################\n",
    "\n",
    "num_trees = 100\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "result = cross_val_score(model, x, y, cv=kfold)\n",
    "print(result.mean())\n",
    "### Estimated mean Accuracy = 0.766\n",
    "\n",
    "#################### Voting Ensemble for Classification ###############################\n",
    "##### create sub models\n",
    "\n",
    "# estimator = []\n",
    "# estimator.append(('cart', DecisionTreeClassifier()))\n",
    "# estimator.append(('logic', LogisticRegression()))\n",
    "# estimator.append(('svm', SVC()))\n",
    "# model = VotingClassifier(estimator)\n",
    "# result = cross_val_score(model, x, y, cv=kfold)\n",
    "# print(result.mean())\n",
    "### Estimated mean Accuracy = 0.739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Improve Performance with Algorithm Tuning\n",
    "\n",
    "Machine learning models are parameterized so that their behavior can be tuned for a given\n",
    "problem. Models can have many parameters and finding the best combination of parameters can\n",
    "be treated as a search problem. \n",
    "\n",
    "16.1 Machine Learning Algorithm Parameters\n",
    "\n",
    "Algorithm tuning is a final step in the process of applied machine learning before finalizing your model. It is sometimes called hyperparameter optimization where the algorithm parameters\n",
    "are referred to as hyperparameters, whereas the coeficients found by the machine learning\n",
    "algorithm itself are referred to as parameters. Optimization suggests the search-nature of the\n",
    "problem. We can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem. \n",
    "\n",
    "16.2 Grid Search Parameter Tuning\n",
    "\n",
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a\n",
    "model for each combination of algorithm parameters specified in a grid. The example below evaluates different alpha values for the Ridge Regression algorithm on the diabetes dataset performing a grid search using the GridSearchCV class.\n",
    "\n",
    "16.3 Random Search Parameter Tuning\n",
    "\n",
    "Random search is an approach to parameter tuning that will sample algorithm parameters from\n",
    "a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed\n",
    "and evaluated for each combination of parameters chosen. A random search for algorithm parameters using the RandomizedSearchCV class.\n",
    "The example below evaluates different random alpha values between 0 and 1 for the Ridge Regression algorithm on the diabetes dataset.  total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27961712703051084\n",
      "0.9779895119966027\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "#################### Grid search for algorithm Tunning ###############################\n",
    "# alph = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "# param_grid = dict(alpha=alph)\n",
    "# model = Ridge()\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "# grid.fit(x, y)\n",
    "# print(grid.best_score_) # best_score = 0.279 ## the optimal score achieved \n",
    "# print(grid.best_estimator_.alpha) # alpha = 1.0 parameters in the grid that achieved that score\n",
    "\n",
    "#################### Randomized for algorithm Tunning ###############################\n",
    "param_grid = {'alpha': uniform()}\n",
    "model = Ridge()\n",
    "search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, \n",
    "                            random_state=7)\n",
    "search.fit(x, y)\n",
    "print(search.best_score_) # 0.279\n",
    "print(search.best_estimator_.alpha) # 0.977  optimal alpha value near 1.0 is discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Save and Load ML Models\n",
    "\n",
    "Finding an accurate machine learning model is not the end of the project. We need to save model to file and load it later in order to make predictions. \n",
    "\n",
    "17.1 Finalize Your Model with pickle\n",
    "\n",
    "Pickle is the standard way of serializing objects in Python. We can use the pickle\n",
    "operation to serialize machine learning algorithms and save the serialized format to a file. Later We can load this file to deserialize the model and use it to make new predictions. \n",
    "The example demonstrates how to train model on dataset, save model and load it make prediction on unseen data.\n",
    "\n",
    "17.2 Finalize Your Model with Joblib\n",
    "\n",
    "The Joblib library is part of the SciPy ecosystem and provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently. This can be useful for some machine learning algorithms that require a lot of parameters or store the entire dataset (e.g. k-Nearest Neighbors). \n",
    "The example below demonstrates how you can train a logistic regression model on the diabetes dataset, save the model to file using Joblib and load it to make predictions on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from pickle import dump\n",
    "# from pickle import load\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.externals.joblib import load\n",
    "\n",
    "rawdata = read_csv('C:/Users/Satish/python_files/diabetes.csv')\n",
    "array = rawdata.values\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "##### split the dataset and train the model \n",
    "x_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#################### Save model using Pickle ###############################\n",
    "##### save model on local disk\n",
    "# filename = 'final_model.sav'\n",
    "# dump(model, open(filename, 'wb'))\n",
    "\n",
    "#### load the model from disk and make prediction\n",
    "# loaded_model = load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(x_test, y_test)\n",
    "# print(result)\n",
    "\n",
    "#################### Save model using Joblib ###############################\n",
    "##### save model on local disk\n",
    "filename = 'final_model23.sav'\n",
    "dump(model, filename)\n",
    "\n",
    "#### load the model from disk and make prediction\n",
    "loaded_model = load(filename)\n",
    "result = loaded_model.score(x_test, y_test)\n",
    "print(result)\n",
    "\n",
    "## the model saves a file as final_model23.sav on local disk and creates one file for each \n",
    "## NumPy array in the model \n",
    "## After the model is loaded an estimate of the accuracy of the model on unseen data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
