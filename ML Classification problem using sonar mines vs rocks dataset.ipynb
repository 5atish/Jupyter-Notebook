{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification ML problem with Sonar Mines vs Rock Datadet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Satish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048\n",
      "[[25  0]\n",
      " [ 4 13]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          M       0.86      1.00      0.93        25\n",
      "          R       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.92      0.90      0.90        42\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "############################## Load Libraries, dataset #####################################\n",
    "\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "dataset = read_csv('C:/Users/Satish/python_files/sonar.csv', header=None)\n",
    "\n",
    "\n",
    "############################## Analyse the Data #####################################\n",
    "# print(dataset.head(10))\n",
    "# print(dataset.shape)\n",
    "# print(dataset.describe())\n",
    "# print(dataset.dtypes)\n",
    "# print(dataset.groupby(60).size())\n",
    "\n",
    "############################## Visualize the Data #####################################\n",
    "########### Unimodal Data Visualizations\n",
    "\n",
    "# dataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1) \n",
    "### From histogram we can see lot Gaussian and Exponential distribution of Data\n",
    "\n",
    "# dataset.plot(kind='density', sharex=False, subplots=True, layout=(8,8), legend=False, fontsize=1)\n",
    "# dataset.plot(kind='box', sharex=False, subplots=True, layout=(8,8), sharey=False, fontsize=1)\n",
    "# pyplot.show()\n",
    "\n",
    "########## Multimodal Data Visualizations\n",
    "# fig = pyplot.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(dataset.corr(), vmin=1, vmax=1, interpolation = 'none')\n",
    "# fig.colorbar(cax)\n",
    "# pyplot.show()\n",
    "### This shows interesting fact that attribute next to each other are generally correlated   \n",
    "\n",
    "############################## Split up dataset into train and test set #####################\n",
    "\n",
    "array = dataset.values\n",
    "x = array[:,0:60]\n",
    "y = array[:,60]\n",
    "test_size = 0.2\n",
    "seed = 27\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)\n",
    "num_fold = 10\n",
    "scoring = 'accuracy'\n",
    "############################## Build the model #####################################\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "############################## Evaluate each model #####################################\n",
    "# results = []\n",
    "# names = []\n",
    "# for name, model in models:\n",
    "#     kfold = KFold(n_splits=num_fold, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     print(name, cv_results.mean(), cv_results.std())\n",
    "### Results suggest both Logistic and KNN worth to further study\n",
    "\n",
    "\n",
    "############################## Compare the model #######################################\n",
    "# fig = pyplot.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# pyplot.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# pyplot.show()\n",
    "\n",
    "######################## Standardize the dataset & build model ##############################\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "# for name, model in pipelines:\n",
    "#     kfold = KFold(n_splits=num_fold, random_state=seed)\n",
    "#     cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     print(name, cv_results.mean(), cv_results.std())\n",
    "### from results we can see that standardization has lifted the skills of SVM to be the most accurate algo tested\n",
    "\n",
    "############################## Compare the Scaled model #######################################\n",
    "# fig = pyplot.figure()\n",
    "# fig.suptitle('Scaled Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# pyplot.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# pyplot.show()\n",
    "\n",
    "############################## Algorithm Tunning #######################################\n",
    "########### Tunning KNN\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "scaledx = scaler.transform(x_train)\n",
    "kvalues = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "param_grid = dict(n_neighbors=kvalues)\n",
    "model = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=num_fold, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring=scoring)\n",
    "grid_result = grid.fit(scaledx, y_train)\n",
    "# print(\"Best: \", grid_result.best_score_, \"using: \", grid_result.best_params_)\n",
    "mean = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "# for mean, std, param in zip(mean, stds, params):\n",
    "#     print(mean, std, param)\n",
    "### After Tunning KNN algo for different k values, It shows high score = 0.843  with k value = 1\n",
    "\n",
    "########### Tunning of SVM with different kernal values and C values\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "rescaledx = scaler.transform(x_train)\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 1.9, 2.0]\n",
    "kernal_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernal_values)\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits=num_fold, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring=scoring)\n",
    "grid_result = grid.fit(rescaledx, y_train)\n",
    "# print(\"Best: \", grid_result.best_score_, \"using: \", grid_result.best_params_)\n",
    "mean = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "# for mean, std, param in zip(mean, stds, params):\n",
    "#     print(mean, std, param)\n",
    "### After Tunning SVM algo for different C and kernal values, It shows high score(0.8313) for C=1.3 & kernel=rbf\n",
    "\n",
    "################## Improve the performance of model using Ensamble methods ##########################\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier()))\n",
    "ensembles.append(('GBM', GradientBoostingClassifier()))\n",
    "ensembles.append(('RF', RandomForestClassifier()))\n",
    "ensembles.append(('ET',ExtraTreesClassifier()))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_fold, random_state=seed)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "#     print(name, cv_results.mean(), cv_results.std())\n",
    "### After boosting technique , we can see that result of GBM worth to further study  \n",
    "\n",
    "############################## Compare the Scaled model #######################################\n",
    "# fig = pyplot.figure()\n",
    "# fig.suptitle('Ensemble Algorothm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# pyplot.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# pyplot.show()\n",
    "\n",
    "############################## Finalize the model #######################################\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "rescaledx = scaler.transform(x_train)\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(rescaledx, y_train)\n",
    "\n",
    "rescaled_testx = scaler.transform(x_test)\n",
    "prediction = model.predict(rescaled_testx)\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(y_test,prediction))\n",
    "print(classification_report(y_test, prediction))\n",
    "\n",
    "############# We achive accuracy of 90% ###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
