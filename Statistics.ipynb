{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 Multivariate Statistics\n",
    "\n",
    "\n",
    "17.2 Expected Value and Mean\n",
    "\n",
    "In probability, the average value of some random variable X is called the expected value or the expectation. The expected value uses the notation E with square brackets around the name of the variable. It is calculated as the probability weighted sum of values that can be drawn.for example \n",
    "\n",
    "                            [E] = sum of ( x1 * p1,x2 * p2,x3 * p3,....,xn * pn)  \n",
    "                            \n",
    "In simple cases, such as the flipping of a coin or rolling a dice, the probability of each event is just as likely. Therefore, the expected value can be calculated as the sum of all values multiplied by the reciprocal of the number of values.                        \n",
    "\n",
    "                            [E] = (1/n) sum of (x1,x2,x3,....,xn)\n",
    "                            \n",
    "In statistics, the mean, or more technically the arithmetic mean or sample mean, can be estimated from a sample of examples drawn from the domain. It is confusing because mean, average, and expected value are used interchangeably. In the abstract, the mean is denoted by the lower case Greek letter mu and is calculated from the sample of observations, rather than all possible values.                            \n",
    "\n",
    "                            mu = (1/n) sum of (x1,x2,x3,....,xn)  = p(x) sum of (x) \n",
    "                            \n",
    "The arithmetic mean can be calculated for a vector or matrix in NumPy by using the mean() function.\n",
    "The mean function can calculate the row or column means of a matrix by specifying the axis argument and the value 0 or 1 respectively.\n",
    "\n",
    "\n",
    "17.3 Variance and Standard Deviation\n",
    "\n",
    "In probability, the variance of some random variable X is a measure of how much values in the distribution vary on average with respect to the mean. The variance is denoted as the function Var() on the variable and calculated as the average squared difference of each value in the distribution from the expected value. Or the expected squared difference from the expected value.\n",
    "\n",
    "                            Var[X] = E[(X  E[X])^2] \n",
    "                            \n",
    "Assuming the expected value of the variable has been calculated (E[X]), the variance of the random variable can be calculated as the sum of the squared difference of each example from the expected value multiplied by the probability of that value.                            \n",
    "\n",
    "                Var[X] = sum of{(p(X1) * (X1 - E[X])^2), (p(X2) * (X2 - E[X])^2),...., (p(Xn) * (Xn - E[X])^2)} \n",
    "                \n",
    "If the probability of each example in the distribution is equal, variance calculation can drop the individual probabilities and multiply the sum of squared di\u000b",
    "erences by the reciprocal of the number of examples in the distribution.                \n",
    "\n",
    "                Var[X] = (1/n) sum of{((X1 - E[X])^2),((X2 - E[X])^2),....,((Xn - E[X])^2)} \n",
    "\n",
    "In statistics, the variance can be estimated from a sample of examples drawn from the domain. The sum of the squared differences is multiplied by the reciprocal of the number of examples minus 1 to correct for a bias (bias is related to a deeper discussion on degrees of freedom and I refer you to references at the end of the lesson).\n",
    "\n",
    "                                     n   \n",
    "                Var[X] = (1/(n-1)) sumof (Xi - mean(X))^2\n",
    "                                    i= 1\n",
    "                                    \n",
    "In NumPy, the variance can be calculated for a vector or a matrix using the var() function. By default, the var() function calculates the population variance. To calculate the sample variance, you must set the ddof argument to the value 1.                                      \n",
    " The standard deviation is calculated as the square root of the variance and is denoted as  lowercase s.\n",
    "\n",
    "                s = sqrt(Var[X})\n",
    "                \n",
    "NumPy also provides a function for calculating the standard deviation directly via the std() function. As with the var() function, the ddof argument must be set to 1 to calculate the unbiased sample standard deviation and column and\n",
    "row standard deviations can be calculated by setting the axis argument to 0 and 1 respectively.\n",
    "                \n",
    "                \n",
    "17.4 Covariance and Correlation\n",
    "\n",
    "The covariance matrix is a square and symmetric matrix that describes the covariance between two or more random variables. The diagonal of the covariance matrix are the variances of each of the random variables, as such it is often called the variance-covariance matrix. A covariance matrix is a generalization of the covariance of two variables and captures the way in which all variables in the dataset may change together. The covariance matrix is denoted as the uppercase Greek letter Sigma. \n",
    "\n",
    "                Sigma = E[(X - E[X]) * (Y - E[Y])]     =  cov(Xi, Yj)    i =  1,2,..n ; j = 1,2,...,m\n",
    "                \n",
    "X is a matrix where each column represents a random variable. The covariance matrix provides a useful tool for separating the structured relationships in a matrix of random variables. This can be used to decorrelate variables or applied as a transform to other variables. It is a key element used in the Principal Component Analysis data reduction method.\n",
    "\n",
    "The covariance matrix can be calculated in NumPy using the cov() function. The cov() function can be called with a single 2D array where each sub-array contains a feature (e.g. column). If this function is called with your data defined in a normal matrix format (rows then columns), then a transpose of the matrix will need to be provided to the function in order to correctly calculate the covariance of the columns.\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.25 0.75]\n",
      " [0.25 0.5  0.25]\n",
      " [0.75 0.25 1.3 ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from numpy import std\n",
    "from numpy import cov\n",
    "\n",
    "v = array([1,2,3,4,5,6])\n",
    "#print(mean(v)) # calculate mean function for vector\n",
    "#print(var(v, ddof = 1)) # calculate variance function for vector\n",
    "#print(std(v, ddof = 1)) # calculate variance function for vector\n",
    "\n",
    "M = array([\n",
    "[1,2,3,4,5,6],\n",
    "[11,32,53,42,59,65]])\n",
    "#print(mean(M, axis=0)) # calculate mean function for column of matrix M\n",
    "#print(mean(M, axis=1)) # calculate mean function for row of matrix M\n",
    "\n",
    "#print(var(M, ddof=1, axis=0)) # calculate variance function for column of matrix M\n",
    "#print(var(M, ddof=1, axis=1)) # calculate vvariance function for row of matrix M\n",
    "\n",
    "#print(std(M, ddof=1, axis=0)) # calculate variance function for column of matrix M\n",
    "#print(std(M, ddof=1, axis=1)) # calculate vvariance function for row of matrix M\n",
    "\n",
    "X = array([\n",
    "[1, 5, 8],\n",
    "[3, 5, 11],\n",
    "[2, 4, 9],\n",
    "[3, 6, 10],\n",
    "[1, 5, 10]])\n",
    "\n",
    "Sigma = cov(X.T) # calculate covaraince matrix\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Principal Component Analysis\n",
    "\n",
    "An important machine learning method for dimensionality reduction is called Principal Component Analysis which uses simple matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions.\n",
    "\n",
    "18.2 What is Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis, or PCA is a method for reducing the dimensionality of data. It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, while retaining the essence of the original data. \n",
    "PCA is an operation applied to a dataset, represented by an n x m matrix A that results in a projection of A which we will call B. Let's walk through the steps of this operation.\n",
    "\n",
    "                            a(1;1) a(1;2)\n",
    "                        A = a(2;1) a(2;2)  \n",
    "                            a(3;1) a(3;2) \n",
    "                            \n",
    "                        B = PCA(A)    \n",
    "            \n",
    "step 1 : calculate the mean values of each column.\n",
    "\n",
    "                        M = mean(A)  ## for each column\n",
    "                        \n",
    "step 2 : center the values in each column by subtracting the mean column value.\n",
    "\n",
    "                        C = A - M\n",
    "                        \n",
    "Step 3 : to calculate the covariance matrix of the centered matrix C. \n",
    "\n",
    "                            V = cov(C)                    \n",
    "\n",
    "         Correlation is a normalized measure of the amount and direction (positive or negative) that two columns change together. A covariance matrix is a calculation of covariance of a given matrix with covariance scores for every column with every other column, including itself. \n",
    "\n",
    "Step 4 : to the eigendecomposition of the covariance matrix V. This results in a list of eigenvalues and a list of eigenvectors. \n",
    "        \n",
    "                        eigen_values, eigen_vectors = eig(V) \n",
    "                        \n",
    "The eigenvectors represent the directions or components for the reduced subspace of B, whereas the eigenvalues represent the magnitudes for the directions. \n",
    "\n",
    "A total of m or less components must be selected to comprise the chosen subspace. Ideally, we would select k eigenvectors, called principal components, that have the k largest eigenvalues.\n",
    "\n",
    "                       B = select(values, vecotrs)\n",
    "                       \n",
    "                       \n",
    "Other matrix decomposition methods can be used such as Singular-Value Decomposition, or SVD. As such, generally the values are referred to as singular values and the vectors of the subspace are referred to as principal components. Once chosen, data can be projected into the subspace via matrix multiplication. \n",
    "\n",
    "                      p = B^T . A\n",
    "                      \n",
    "Where A is the original data that we wish to project, B^T s the transpose of the chosen\n",
    "principal components and P is the projection of A. This is called the covariance method for\n",
    "calculating the PCA                      \n",
    "\n",
    "\n",
    "18.4 Principal Component Analysis in scikit-learn\n",
    "\n",
    "We can calculate a Principal Component Analysis on a dataset using the PCA() class in the scikit-learn library. The benefit of this approach is that once the projection is calculated, it can be applied to new data again and again quite easily. \n",
    "When creating the class, the number of\n",
    "components can be specified as a parameter. The class is first fit on a dataset by calling the fit() function, and then the original dataset or other data can be projected into a subspace with the chosen number of dimensions by calling the transform() function. Once fit, the singular values and principal components can be accessed on the PCA class via the explained variance and components attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "[8.00000000e+00 2.25080839e-33]\n",
      "[[-2.82842712e+00  2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00 -2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "\n",
    "M = mean(A.T, axis = 1) # calculate column wise mean\n",
    "#print(M)\n",
    "C = A - M # calculate center column by subtracting column means\n",
    "#print(C)\n",
    "V = cov(C.T) # calculate covariance matrix of centered matrix\n",
    "#print(V)\n",
    "values, vectors = eig(V) # Factorize covarince matrix\n",
    "#print(values)\n",
    "#print(vectors)\n",
    "\n",
    "P = vectors.T.dot(C.T) # project data\n",
    "\n",
    "#print(P.T) # from result it suggesting that we could project our 3 X 2 matrix onto a 3 x 1 \n",
    "           # matrix with little loss.\n",
    "      \n",
    "        \n",
    "###########principal component analysis with scikit-learn####################\n",
    "\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "\n",
    "pca = PCA(2) # create the transform \n",
    "pca.fit(A) # fit transform\n",
    "\n",
    "#access values and vectors\n",
    "print(pca.components_) \n",
    "print(pca.explained_variance_)\n",
    "\n",
    "B  = pca.transform(A) # transform data\n",
    "print(B)\n",
    "\n",
    "## with with some very minor floating point rounding that we achieve the same principal \n",
    "##components, singular values, and projection as in the previous example.                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Linear Regression\n",
    "\n",
    "19.4 Linear Regression Dataset\n",
    "\n",
    "19.5 Solve via Inverse\n",
    "\n",
    "To solve the regression problem using the matrix inverse. For given input x, the coefficients can be calculated by below equation which then multiply by x will provide y.\n",
    "\n",
    "            b = (X^T \u0001 X)^-1 \u0001 X^T \u0001 y \n",
    "            \n",
    "This can be achive in NumPy using the inv() function for calculating the matrix inverse.\n",
    "    i.e      b = inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    \n",
    "Once the coefficients are calculated, we can use them to predict outcomes given X. i.e \n",
    "            yhat = X.dot(b)\n",
    "           \n",
    "A problem with this approach is the matrix inverse that is both computationally expensive\n",
    "and numerically unstable. An alternative approach is to use a matrix decomposition to avoid\n",
    "this operation. \n",
    "\n",
    "\n",
    "19.7 Solve via SVD and Pseudoinverse\n",
    "\n",
    "Singular-Value Decomposition, or SVD for short, is a matrix decomposition method like QR decomposition.\n",
    "\n",
    "            X = U \u0001 E \u0001 V^T\n",
    "            \n",
    "Where X is the real n x m matrix that we wish to decompose, U is a m x m matrix, E is an m x n diagonal matrix, and V^T is the transpose of an n x n matrix. Unlike the QR decomposition, all matrices have a singular-value-decomposition\n",
    "Once decomposed, the coefficients can be found by calculating the pseudoinverse of the input matrix X and multiplying that by the output vector y.\n",
    "\n",
    "            b = X^+ \u0001 y \n",
    "            \n",
    "Where the pseudoinverse X^+ is calculated as  X^+ = U \u0001 D^+ \u0001 V^T\n",
    "NumPy provides the function pinv() to calculate the pseudoinverse directly.\n",
    "\n",
    "\n",
    "\n",
    "19.8 Solve via Convenience Function\n",
    "\n",
    "NumPy provides a convenience function named lstsq() that solves the linear least squares function using the SVD approach. The function takes as input the X matrix and y vector and returns the b coefficients as well as residual errors, the rank of the provided X matrix and the singular values. \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00233226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import pinv\n",
    "from numpy.linalg import lstsq\n",
    "from matplotlib import pyplot\n",
    "\n",
    "data = array([\n",
    "[0.05, 0.12],\n",
    "[0.18, 0.22],\n",
    "[0.31, 0.35],\n",
    "[0.42, 0.38],\n",
    "[0.5, 0.49]])\n",
    "\n",
    "x, y = data[:,0], data[:,1] # split data into input and output\n",
    "#print(y)\n",
    "X = x.reshape((len(x),1))\n",
    "#print(x)\n",
    "\n",
    "#pyplot.scatter(X,y) # scatter plot of the dataset is created \n",
    "#pyplot.show() # shows straight line cannot fit this dataset.\n",
    "\n",
    "\n",
    "############## The Regression problem solve by matrix inversion. #####################\n",
    "\n",
    "#b  = inv(X.T.dot(X)).dot(X.T).dot(y) # calculate the coefficient using x and y\n",
    "#print(b)\n",
    "\n",
    "#yhat = X.dot(b) # make prediction using x and coefficient\n",
    "\n",
    "#pyplot.scatter(X, y) # scatter plot of dataset\n",
    "#pyplot.plot(X, yhat, color=\"red\") # line plot for model\n",
    "#pyplot.show()\n",
    "\n",
    "\n",
    "############## The Regression problem solve via SVD and Pseudoinverse. ####################\n",
    "\n",
    "#b = pinv(X).dot(y) # calculate the coefficient using x and y\n",
    "#print(b)\n",
    "\n",
    "#yhat = X.dot(b)  # make prediction using x and coefficient\n",
    "\n",
    "# pyplot.scatter(X, y) # scatter plot of dataset\n",
    "# pyplot.plot(X, yhat, color=\"red\") # line plot for model\n",
    "# pyplot.show()\n",
    "\n",
    "\n",
    "############## The Regression problem solve via Convenience Function. ####################\n",
    "\n",
    "b, residuals, rank, s = lstsq(X, y)  # calculate the coefficient using x and y\n",
    "print(b)\n",
    "\n",
    "yhat = X.dot(b)  # make prediction using x and coefficient\n",
    "\n",
    "pyplot.scatter(X, y) # scatter plot of dataset\n",
    "pyplot.plot(X, yhat, color=\"red\") # line plot for model\n",
    "pyplot.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
