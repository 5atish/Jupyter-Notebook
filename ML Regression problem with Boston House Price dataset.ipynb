{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of the Boston House Price dataset\n",
    "\n",
    "Dataset has 506 instances to work with and has 14 attributes including the output attribute MEDV.\n",
    "\n",
    "We can see that many of the attributes have a strong correlation (e.g.Attributes > 0:70 or Attributes < 0:70). For example:\n",
    "- NOX and INDUS with 0.77.\n",
    "- DIS and INDUS with -0.71.\n",
    "- TAX and INDUS with 0.72.\n",
    "- AGE and NOX with 0.73.\n",
    "- DIS and NOX with -0.78.\n",
    "It also looks like LSTAT has a good negative correlation with the output variable MEDV with\n",
    "value of -0.74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "Best:  -10.140050179779665 using:  {'n_estimators': 350}\n",
      "8.158339502671137\n"
     ]
    }
   ],
   "source": [
    "############################## Load Libraries #####################################\n",
    "from numpy import arange\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "############################## Import Dataset #####################################\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B','LSTAT', 'MEDV']\n",
    "dataset = read_csv('C:/Users/Satish/python_files/Bhousing.csv', delim_whitespace=True, names=names)\n",
    "\n",
    "############################## Import Dataset #####################################\n",
    "# print(dataset.head(5))\n",
    "print(dataset.shape)\n",
    "set_option('precision',2)\n",
    "# print(dataset.describe())\n",
    "# print(dataset.corr(method='pearson'))\n",
    "# print(dataset.skew())\n",
    "# print(dataset.dtypes)\n",
    "\n",
    "############################## Data Visualize #####################################\n",
    "\n",
    "\n",
    "# dataset.hist(sharex=False, sharey=False, xlabelsize= 1, ylabelsize= 1)\n",
    "### some attributes may have an exponential distribution, such as CRIM, ZN, Dis, Age\n",
    "### some attributes may have an Binomial distribution, such as Lstat, RAD\n",
    "\n",
    "# dataset.plot(kind='density', subplots=True, sharex=False, fontsize=1, layout=(4,4))\n",
    "### looks like NOX, RM and LSTAT may be skewed Gaussian distributions.\n",
    "\n",
    "# dataset.plot(kind='box', subplots=True, sharex=False, sharey=False, layout=(4,4), fontsize=8)\n",
    "\n",
    "# scatter_matrix(dataset)\n",
    "# pyplot.show()\n",
    "\n",
    "#correlationmatrix\n",
    "# fig = pyplot.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\n",
    "# fig.colorbar(cax)\n",
    "# ticks = numpy.arange(0,14,1)\n",
    "# ax.set_xticks(ticks)\n",
    "# ax.set_yticks(ticks)\n",
    "# ax.set_xticklabels(names)\n",
    "# ax.set_yticklabels(names)\n",
    "# pyplot.show()\n",
    "\n",
    "### The dark yellow color shows positive correlation whereas the dark blue color shows negative\n",
    "### correlation. It also suggest that dark yellow and blue good candidates for removal to \n",
    "### better improve accuracy of models.\n",
    "\n",
    "############################## Create train and test Dataset #####################################\n",
    "\n",
    "array = dataset.values\n",
    "# print(array[0:10,0:13])\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "# print(y[:5])\n",
    "\n",
    "validation_size = 0.20\n",
    "num_folds = 10\n",
    "seed = 17\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=validation_size, random_state = seed)\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "############################## Build the model #####################################\n",
    "models = []\n",
    "models.append(('LR',LinearRegression()))\n",
    "models.append(('LSS', Lasso()))\n",
    "models.append(('EN', ElasticNet()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('SVM', SVR()))\n",
    "\n",
    "############################## Evaluate each model #####################################\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "#     print(name, cv_results.mean(), cv_results.std())\n",
    "#### LR and CART has lowest MSE\n",
    "\n",
    "############################## Compare the model #######################################\n",
    "\n",
    "# fig = pyplot.figure()\n",
    "# fig.suptitle('AlgorithmComparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# pyplot.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# pyplot.show()\n",
    "\n",
    "############################## Standardize the dataset #####################################\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))                 \n",
    "pipelines.append(('ScaledLSS', Pipeline([('Scaler', StandardScaler()),('LSS', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVR())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "#     print(name, cv_results.mean(), cv_results.std())\n",
    "### we can see that scaling has effect on KNN, bringing down the error lower than other models    \n",
    "   \n",
    "############################## Compare the Scaled model #######################################    \n",
    "# fig = pyplot.figure()\n",
    "# fig.suptitle('Scaled Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# pyplot.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# pyplot.show()    \n",
    "###KNN has both a tight distribution of error and has the lowest score.\n",
    "\n",
    "############################## Improve result with Tuning #######################################\n",
    "### KNN Algo tunning\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "rescaledx = scaler.transform(x_train)\n",
    "k_values = numpy.array([1,3,5,7,9,11,13,15,17,19,21])\n",
    "param_grid = dict(n_neighbors = k_values)\n",
    "model = KNeighborsRegressor()\n",
    "kfold = KFold(n_splits=num_folds,random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring=scoring)\n",
    "grid_result = grid.fit(rescaledx, y_train)\n",
    "# print(\"Best Score: \",grid_result.best_score_, \"Best param: \", grid_result.best_params_)\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(mean, stdev, param)\n",
    "### we can see that KNN = 3 provide MSE =  -20.659 wich is best so far now.\n",
    "\n",
    "################## Improve the performance of model using Ensamble methods ##########################\n",
    "ensembles = []\n",
    "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()), ('AB', AdaBoostRegressor())])))\n",
    "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()), ('GBM', GradientBoostingRegressor())])))\n",
    "ensembles.append(('ScaledRM', Pipeline([('Scaler', StandardScaler()), ('RM', RandomForestRegressor())])))\n",
    "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()), ('ET', ExtraTreesRegressor())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "#     print(name, cv_results.mean(), cv_results.std())\n",
    "\n",
    "# fig = pyplot.figure()\n",
    "# fig.suptitle('Scaled Ensemble Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# pyplot.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# pyplot.show() \n",
    "### It looks like Gradient Boosting has a better mean score.\n",
    "\n",
    "############################## Tune Ensemble Mothods #######################################\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "rescaledx = scaler.transform(x_train)\n",
    "param_grid = dict(n_estimators = numpy.array([50,100,150,200,250,300,350,400]))\n",
    "model  = GradientBoostingRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring=scoring)\n",
    "grid_result = grid.fit(rescaledx, y_train)\n",
    "\n",
    "#### Summerize the best performance and check how performance changed with esch differenct configuration\n",
    "\n",
    "print(\"Best: \", grid_result.best_score_, \"using: \", grid_result.best_params_)\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "# for mean, std, param in zip(means, stds, params):\n",
    "#     print(mean, std, param)\n",
    "### We can see that the best configuration was n estimators=350 resulting in a mean squared error of -10.14\n",
    "    \n",
    "############################## Finalize the model #######################################\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "rescaledx = scaler.transform(x_train)\n",
    "model = GradientBoostingRegressor(random_state=seed, n_estimators=350)\n",
    "model.fit(rescaledx, y_train)\n",
    "\n",
    "############################## Transform the test dataset #######################################\n",
    "rescaledtest_x = scaler.transform(x_test)\n",
    "prediction = model.predict(rescaledtest_x)\n",
    "print(mean_squared_error(y_test, prediction))\n",
    "######### mean_squared_error : 8.158"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
